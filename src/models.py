# -*- coding: utf-8 -*-
"""models_11_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kDHFahKuxyu-lkw6nTAlqDF3udfIogmV
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder,LabelEncoder
from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler
from sklearn.model_selection import train_test_split 
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay
from sklearn.metrics import classification_report
import time
from sklearn import metrics
import pickle
from beautifultable import BeautifulTable
import kaggle
import os
import seaborn as sns

import xbosmodel

import hbosmodel

from functions import plot_top_n,my_plot_precision_recall_curve, confusion_matrix_plot,download_drive, auc_plot

import torch
from termcolor import colored




#kamel
def model(dataset_name,df,name_target, n,contamination,number_of_unique,percentage_values,list_of_models):

    if contamination > 0.5:
      contamination = 0.5

    X, y = df.loc[:, df.columns!= name_target], df[name_target]
    seed = 120
    test_size = 0.3
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed,stratify=y)
    #print('X_test:',X_test.shape,'y_test:',y_test.shape)

    train, test = train_test_split(df, test_size=test_size, random_state=seed,stratify=y)

    print(colored('******* Anomaly rate train *******', 'green', attrs=['bold']))
    Anomaly_rate_train = 1.0-len(train.loc[train[name_target]==0])/len(train)
    print(Anomaly_rate_train)

    print(colored('******* Anomaly rate test *******', 'green', attrs=['bold']))  
    Anomaly_rate_test = 1.0-len(test.loc[test[name_target]==0])/len(test)  
    print(Anomaly_rate_test)
    #print(colored('***************************************', 'green', attrs=['bold']))    

    #create a dataframe
    df_all = pd.DataFrame(columns =["method",'TP', 'FP','TN','FN','Accuracy', 'Precision', 'Recall',
                                    'F1_score','AUC','AP','Training Time(s)'])
    index = df_all.index
    index.name = dataset_name

    numb = len(df_all)+1

    #**************************************************************HBOS
    
    if 'HBOS_pyod' in list_of_models:

      print('**************************************************************HBOS_pyod')
      
      from pyod.models.hbos import HBOS
      import time

      model_name_1 = 'HBOS_pyod'
      # train HBOS detector
      clf_name = 'HBOS_pyod'
      clf = HBOS(contamination= contamination)
      start = time.time()
      clf.fit(X_train)

      # get the prediction on the test data
      y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)
      y_test_scores_hbos = clf.decision_function(X_test)  # outlier scores

      stop = time.time()
      train_time_hbos = stop - start
      print(f"Training time: {stop - start}s")

      predictions = [round(value) for value in y_test_pred]
      accuracy = accuracy_score(y_test, predictions)
      accuracy_1 = accuracy * 100.0

      for i in range(0,len(predictions)):
        if predictions[i] > 0.5:
          predictions[i]=1
        else:
          predictions[i]=0

      predictions_1 = predictions

      #AUC score
      auc_1 = metrics.roc_auc_score(y_test, predictions_1)
      # "{:.3%}".format(auc_1)

      # AP score
      average_precision_1 = metrics.average_precision_score(y_test, predictions_1)
      #"{:.3%}".format(average_precision_1)


      # calculate prediction,recall, f1-score
      from sklearn.metrics import f1_score,recall_score,precision_score
      precision = precision_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      recall = recall_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score = f1_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score_1 = np.mean(f1_score)
      precision_1 = np.mean(precision)
      recall_1 = np.mean(recall)

      # evaluate the classification_report
      print(classification_report(y_test, predictions_1))   

      # evaluate the confusion_matrix
      cf_matrix =confusion_matrix(y_test, predictions)

      tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()

      #*******************************************confusion matrix Plot

      confusion_matrix_plot(y_test,predictions_1,model_name_1,contamination,percentage_values)

      #************************************************

      file_name = '/content/'+ f'{model_name_1}' + '.pkl'    
      import joblib
      with open(file_name, 'wb') as f:
        joblib.dump(clf, f)

      # import pickle
      # with open(file_name, 'wb') as f:
      #   pickle.dump(clf, f)

      # # load the model from disk
      # loaded_model = pickle.load(open(filename, 'rb'))
      # result = loaded_model.score(X_test, Y_test)
      # print(result)  
      
      df_all.loc[numb]= [f"{model_name_1}",tn, fp, fn, tp, accuracy_1, precision_1,
                         recall_1,f1_score_1,"{:.3%}".format(auc_1),"{:.2f}".format(average_precision_1),"{:.2f}".format(train_time_hbos)]
      numb = len(df_all)+1

    else:
      pass

    #**********************************************************************KNN

    if 'KNN_pyod' in list_of_models:

      print('*****************************************************************KNN_pyod')

      from pyod.models.knn import KNN 
      import time
      model_name_2 = 'KNN_pyod'
      # train kNN detector
      clf_name = 'KNN_pyod'
      clf = KNN(contamination= contamination)
      start = time.time()

      clf.fit(X_train)

      # get the prediction on the test data
      y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)
      y_test_scores_knn = clf.decision_function(X_test)  # outlier scores

      stop = time.time()
      train_time_knn = stop - start
      print(f"Training time: {stop - start}s")

      predictions = [round(value) for value in y_test_pred]
      accuracy = accuracy_score(y_test, predictions)
      accuracy_2 = accuracy * 100.0

      for i in range(0,len(predictions)):
        if predictions[i] > 0.5:
          predictions[i]=1
        else:
          predictions[i]=0

      predictions_2 = predictions

      #AUC score
      auc_2 = metrics.roc_auc_score(y_test, predictions_2)
      #{:.3%}".format(auc_1)


      # AP score
      average_precision_2 = metrics.average_precision_score(y_test, predictions_2)

      # calculate prediction,recall, f1-score
      from sklearn.metrics import f1_score,recall_score,precision_score
      precision = precision_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      recall = recall_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score = f1_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score_2 = np.mean(f1_score)
      precision_2 = np.mean(precision)
      recall_2 = np.mean(recall)

      # evaluate the classification_report
      print(classification_report(y_test, predictions_2))

      # evaluate the confusion_matrix
      cf_matrix =confusion_matrix(y_test, predictions)
      tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()

      #*******************************************confusion matrix Plot

      confusion_matrix_plot(y_test,predictions_2,model_name_2,contamination,percentage_values)

      #************************************************


      file_name = '/content/'+ f'{model_name_2}' + '.pkl'
      import joblib
      with open(file_name, 'wb') as f:
        joblib.dump(clf, f)

      # import pickle
      # with open(file_name, 'wb') as f:
      #   pickle.dump(clf, f)

      df_all.loc[numb]= [f"{model_name_2}",tn, fp, fn, tp, accuracy_2, precision_2,recall_2,
                         f1_score_2,"{:.3%}".format(auc_2),"{:.2f}".format(average_precision_2),"{:.2f}".format(train_time_knn)]

      numb = len(df_all)+1

    else:
      pass

    #****************************************************************IForest
    
    if 'IForest_pyod' in list_of_models:

      print('*****************************************************************IForest_pyod')

      from pyod.models.iforest import IForest
      import time

      model_name_3 = 'IForest_pyod'

      # train IForest detector
      clf_name = 'IForest_pyod'
      clf = IForest(contamination= contamination)
      start = time.time()
      clf.fit(X_train)

      # get the prediction on the test data
      y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)
      y_test_scores_iforest = clf.decision_function(X_test)  # outlier scores

      stop = time.time()
      train_time_iforest = stop - start
      print(f"Training time: {stop - start}s")

      #***************************************

      predictions = [round(value) for value in y_test_pred]
      accuracy = accuracy_score(y_test, predictions)
      #print("Accuracy: %.2f%%" % (accuracy * 100.0))
      accuracy_3 = accuracy * 100.0

      for i in range(0,len(predictions)):
        if predictions[i] > 0.5:
          predictions[i]=1
        else:
          predictions[i]=0

      predictions_3 = predictions

      #AUC score
      auc_3 = metrics.roc_auc_score(y_test, predictions_3)
      #{:.3%}".format(auc_1)

      # AP score
      average_precision_3 = metrics.average_precision_score(y_test, predictions_3)

      # calculate prediction,recall, f1-score
      from sklearn.metrics import f1_score,recall_score,precision_score
      precision = precision_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      recall = recall_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score = f1_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score_3 = np.mean(f1_score)
      precision_3 = np.mean(precision)
      recall_3 = np.mean(recall)

      # evaluate the classification_report
      print(classification_report(y_test, predictions_3))
  
      # evaluate the confusion_matrix
      cf_matrix =confusion_matrix(y_test, predictions)
      tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()

      #*******************************************confusion matrix Plot

      confusion_matrix_plot(y_test,predictions_3,model_name_3,contamination,percentage_values)

      #************************************************


      file_name = '/content/'+ f'{model_name_3}' + '.pkl'
      import joblib
      with open(file_name, 'wb') as f:
        joblib.dump(clf, f)

      # import pickle
      # with open(file_name, 'wb') as f:
      #   pickle.dump(clf, f)

      df_all.loc[numb]= [f"{model_name_3}",tn, fp, fn, tp, accuracy_3, precision_3,recall_3,
                         f1_score_3,"{:.3%}".format(auc_3),"{:.2f}".format(average_precision_3),"{:.2f}".format(train_time_iforest)]

      numb = len(df_all)+1

    else:
      pass

    #****************************************************************LOF
    if 'LOF_pyod' in list_of_models:

      print('******************************************************************LOF_pyod')
      from pyod.models.lof import LOF
      import time


      model_name_4 = 'LOF_pyod'

      # train LOF detector
      clf_name = 'LOF_pyod'
      clf = LOF(contamination= contamination)
      start = time.time()
      clf.fit(X_train)

      # get the prediction on the test data
      y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)
      y_test_scores_lof = -clf.decision_function(X_test)  # outlier scores

      stop = time.time()
      train_time_lof = stop - start
      print(f"Training time: {stop - start}s")

      #****************************************
      predictions = [round(value) for value in y_test_pred]
      accuracy = accuracy_score(y_test, predictions)
      #print("Accuracy: %.2f%%" % (accuracy * 100.0))
      accuracy_4 = accuracy * 100.0

      for i in range(0,len(predictions)):
        if predictions[i] > 0.5:
          predictions[i]=1
        else:
          predictions[i]=0

      predictions_4 = predictions

      #AUC score
      auc_4 = metrics.roc_auc_score(y_test, predictions_4)
      #{:.3%}".format(auc_1)

      # AP score
      average_precision_4 = metrics.average_precision_score(y_test, predictions_4)

      # calculate prediction,recall, f1-score
      from sklearn.metrics import f1_score,recall_score,precision_score
      precision = precision_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      recall = recall_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score = f1_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score_4 = np.mean(f1_score)
      precision_4 = np.mean(precision)
      recall_4 = np.mean(recall)

      # evaluate the classification_report
      print(classification_report(y_test, predictions_4))

      # evaluate the confusion_matrix
      cf_matrix =confusion_matrix(y_test, predictions)
      tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()

      #*******************************************confusion matrix Plot

      confusion_matrix_plot(y_test,predictions_4,model_name_4,contamination,percentage_values)

      #************************************************


      file_name = '/content/'+ f'{model_name_4}' + '.pkl'
      import joblib
      with open(file_name, 'wb') as f:
        joblib.dump(clf, f)

      # import pickle
      # with open(file_name, 'wb') as f:
      #   pickle.dump(clf, f)

      df_all.loc[numb]= [f"{model_name_4}",tn, fp, fn, tp, accuracy_4, precision_4,recall_4,
                         f1_score_4,"{:.3%}".format(auc_4),"{:.2f}".format(average_precision_4),"{:.2f}".format(train_time_lof)]

      numb = len(df_all)+1

    else:
      pass
      
      
      
      
      
        #****************************************************************LOF
    if 'CBLOF_pyod' in list_of_models:

      print('******************************************************************CBLOF_pyod')
      from pyod.models.cblof import CBLOF
      import time


      model_name_4cblof = 'CBLOF_pyod'

      # train LOF detector
      clf_name = 'CBLOF_pyod'
      clf = CBLOF(contamination= contamination)
      start = time.time()
      clf.fit(X_train)

      # get the prediction on the test data
      y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)
      y_test_scores_cblof = -clf.decision_function(X_test)  # outlier scores

      stop = time.time()
      train_time_cblof = stop - start
      print(f"Training time: {stop - start}s")

      #****************************************
      predictions = [round(value) for value in y_test_pred]
      accuracy = accuracy_score(y_test, predictions)
      #print("Accuracy: %.2f%%" % (accuracy * 100.0))
      accuracy_4cblof = accuracy * 100.0

      for i in range(0,len(predictions)):
        if predictions[i] > 0.5:
          predictions[i]=1
        else:
          predictions[i]=0

      predictions_4cblof = predictions

      #AUC score
      auc_4cblof = metrics.roc_auc_score(y_test, predictions_4cblof)
      #{:.3%}".format(auc_1)

      # AP score
      average_precision_4cblof = metrics.average_precision_score(y_test, predictions_4cblof)

      # calculate prediction,recall, f1-score
      from sklearn.metrics import f1_score,recall_score,precision_score
      precision = precision_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      recall = recall_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score = f1_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score_4cblof = np.mean(f1_score)
      precision_4cblof = np.mean(precision)
      recall_4cblof = np.mean(recall)

      # evaluate the classification_report
      print(classification_report(y_test, predictions_4cblof))

      # evaluate the confusion_matrix
      cf_matrix =confusion_matrix(y_test, predictions)
      tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()

      #*******************************************confusion matrix Plot

      confusion_matrix_plot(y_test,predictions_4cblof,model_name_4cblof,contamination,percentage_values)

      #************************************************


      file_name = '/content/'+ f'{model_name_4cblof}' + '.pkl'
      import joblib
      with open(file_name, 'wb') as f:
        joblib.dump(clf, f)

      # import pickle
      # with open(file_name, 'wb') as f:
      #   pickle.dump(clf, f)

      df_all.loc[numb]= [f"{model_name_4cblof}",tn, fp, fn, tp, accuracy_4cblof, precision_4cblof,recall_4cblof,
                         f1_score_4cblof,"{:.3%}".format(auc_4cblof),"{:.2f}".format(average_precision_4cblof),"{:.2f}".format(train_time_cblof)]

      numb = len(df_all)+1

    else:
      pass

    #****************************************************************XBOS
    if 'XBOS' in list_of_models:

      print('******************************************************************XBOS')
      import time
      #df_2_exist = False

      if number_of_unique != None:
        df_2 = df.copy()
        #df_2_exist = True

        #remove columns with constant numbers or those columns with unique numbers of < number_of_unique
        cols = df_2.columns
        for i in range(len(cols)):
          if cols[i] != name_target:
            m = df_2[cols[i]].value_counts()
            m = np.array(m)
            if len(m) < number_of_unique:
              print(f'len cols {i}:',len(m), 'droped')
              #print('drope')
              column_name = cols[i]
              df_2=df_2.drop(columns= column_name)

        X_2, y_2= df_2.loc[:, df_2.columns!= name_target], df_2[name_target]
        X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size=0.3, random_state=120,stratify=y_2)
          
        model_name_5 = 'XBOS'
        #create XBOS model
        clf = xbosmodel.XBOS(n_clusters=15,max_iter=1)
        start = time.time()
        # train XBOS model
        clf.fit(X_train_2)
        

        #predict model
        y_test_pred = clf.predict(X_test_2)
        y_test_scores_xbos = clf.fit_predict(X_test_2)
        stop = time.time()
        train_time_xbos = stop - start
        print(f"Training time: {stop - start}s")

      else:

        model_name_5 = 'XBOS'
        #create XBOS model
        clf = xbosmodel.XBOS(n_clusters=15,max_iter=1)
        start = time.time()
        # train XBOS model
        clf.fit(X_train)

        #predict model
        y_test_pred = clf.predict(X_test)
        y_test_scores_xbos = clf.fit_predict(X_test)
        stop = time.time()
        train_time_xbos = stop - start
        print(f"Training time: {stop - start}s")

      #****************************************
      predictions = [round(value) for value in y_test_pred]
      accuracy = accuracy_score(y_test, predictions)
      #print("Accuracy: %.2f%%" % (accuracy * 100.0))
      accuracy_5 = accuracy * 100.0

      for i in range(0,len(predictions)):
        if predictions[i] > 0.5:
          predictions[i]=1
        else:
          predictions[i]=0

      predictions_5 =predictions

      #AUC score
      auc_5 = metrics.roc_auc_score(y_test, predictions_5)
      #{:.3%}".format(auc_1)

      # AP score
      average_precision_5 = metrics.average_precision_score(y_test, predictions_5)

      # calculate prediction,recall, f1-score
      from sklearn.metrics import f1_score,recall_score,precision_score
      precision = precision_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      recall = recall_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score = f1_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score_5 = np.mean(f1_score)
      precision_5 = np.mean(precision)
      recall_5 = np.mean(recall)

      # evaluate the classification_report
      print(classification_report(y_test, predictions_5))

      # evaluate the confusion_matrix
      #cf_matrix =confusion_matrix(y_test, predictions)

      tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()

      #*******************************************confusion matrix Plot

      confusion_matrix_plot(y_test,predictions_5,model_name_5,contamination,percentage_values)

      #************************************************


      file_name = '/content/'+ f'{model_name_5}' + '.pkl'
      import joblib
      with open(file_name, 'wb') as f:
        joblib.dump(clf, f)

      # import pickle
      # with open(file_name, 'wb') as f:
      #   pickle.dump(clf, f)

      df_all.loc[numb]= [f"{model_name_5}",tn, fp, fn, tp, accuracy_5, precision_5,recall_5,
                         f1_score_5,"{:.3%}".format(auc_5),"{:.2f}".format(average_precision_5),"{:.2f}".format(train_time_xbos)]

      numb = len(df_all)+1

    else:
      pass
      
      
      
      
      
    
    
    
    if 'HBOS' in list_of_models:

      print('******************************************************************HBOS')
      import time
      #df_2_exist = False

      if number_of_unique != None:
        df_2 = df.copy()
        #df_2_exist = True

        #remove columns with constant numbers or those columns with unique numbers of < number_of_unique
        cols = df_2.columns
        for i in range(len(cols)):
          if cols[i] != name_target:
            m = df_2[cols[i]].value_counts()
            m = np.array(m)
            if len(m) < number_of_unique:
              print(f'len cols {i}:',len(m), 'droped')
              #print('drope')
              column_name = cols[i]
              df_2=df_2.drop(columns= column_name)

        X_2, y_2= df_2.loc[:, df_2.columns!= name_target], df_2[name_target]
        X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size=0.3, random_state=120,stratify=y_2)
          
        model_name_5h = 'HBOS'
        #create HBOS model
        clf = hbosmodel.HBOS()
        start = time.time()
        # train HBOS model
        clf.fit(X_train_2)
        

        #predict model
        y_test_pred = clf.predict(X_test_2)
        y_test_scores_hbos = clf.fit_predict(X_test_2)
        stop = time.time()
        train_time_hbos = stop - start
        print(f"Training time: {stop - start}s")

      else:

        model_name_5h = 'HBOS'
        #create HBOS model
        clf = hbosmodel.HBOS()
        start = time.time()
        # train HBOS model
        clf.fit(X_train)

        #predict model
        y_test_pred = clf.predict(X_test)
        y_test_scores_hbos = clf.fit_predict(X_test)
        stop = time.time()
        train_time_hbos = stop - start
        print(f"Training time: {stop - start}s")

      #****************************************
      predictions = [round(value) for value in y_test_pred]
      accuracy = accuracy_score(y_test, predictions)
      #print("Accuracy: %.2f%%" % (accuracy * 100.0))
      accuracy_5h = accuracy * 100.0

      for i in range(0,len(predictions)):
        if predictions[i] > 0.5:
          predictions[i]=1
        else:
          predictions[i]=0

      predictions_5h =predictions

      #AUC score
      auc_5h = metrics.roc_auc_score(y_test, predictions_5h)
      #{:.3%}".format(auc_1)

      # AP score
      average_precision_5h = metrics.average_precision_score(y_test, predictions_5)

      # calculate prediction,recall, f1-score
      from sklearn.metrics import f1_score,recall_score,precision_score
      precision = precision_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      recall = recall_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score = f1_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score_5h = np.mean(f1_score)
      precision_5h = np.mean(precision)
      recall_5h = np.mean(recall)

      # evaluate the classification_report
      print(classification_report(y_test, predictions_5))

      # evaluate the confusion_matrix
      #cf_matrix =confusion_matrix(y_test, predictions)

      tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()

      #*******************************************confusion matrix Plot

      confusion_matrix_plot(y_test,predictions_5h,model_name_5h,contamination,percentage_values)

      #************************************************


      file_name = '/content/'+ f'{model_name_5h}' + '.pkl'
      import joblib
      with open(file_name, 'wb') as f:
        joblib.dump(clf, f)

      # import pickle
      # with open(file_name, 'wb') as f:
      #   pickle.dump(clf, f)

      df_all.loc[numb]= [f"{model_name_5h}",tn, fp, fn, tp, accuracy_5h, precision_5h,recall_5h,
                         f1_score_5h,"{:.3%}".format(auc_5h),"{:.2f}".format(average_precision_5h),"{:.2f}".format(train_time_hbos)]

      numb = len(df_all)+1

    else:
      pass

    #**********************************************************************KNN_sklearn
    if 'KNN_sklearn' in list_of_models:

      print('*****************************************************************KNN from sklearn lib')
      
      from sklearn.neighbors import KNeighborsClassifier
      import time

      model_name_6 = 'KNN_sklearn'
      # train knn detector
      neigh = KNeighborsClassifier(n_neighbors=3)
      start = time.time()
      neigh.fit(X_train,y_train)

      # get the prediction on the test data
      y_test_pred_6 = neigh.predict(X_test)

      stop = time.time()
      train_time_knn_sklearn = stop - start
      print(f"Training time: {stop - start}s")

      #*****************************************************
      predictions = [round(value) for value in y_test_pred_6]
      accuracy = accuracy_score(y_test, predictions)
      #print("Accuracy: %.2f%%" % (accuracy * 100.0))
      accuracy_6 = accuracy * 100.0

      for i in range(0,len(predictions)):
        if predictions[i] > 0.5:
          predictions[i]=1
        else:
          predictions[i]=0

      predictions_6 = predictions

      #AUC score
      auc_6 = metrics.roc_auc_score(y_test, predictions_6)
      # "{:.3%}".format(auc_6)

      # AP score
      average_precision_6 = metrics.average_precision_score(y_test, predictions_6)

      # calculate prediction,recall, f1-score
      from sklearn.metrics import f1_score,recall_score,precision_score
      precision = precision_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      recall = recall_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score = f1_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score_6 = np.mean(f1_score)
      precision_6 = np.mean(precision)
      recall_6 = np.mean(recall)

      # evaluate the classification_report
      print(classification_report(y_test, predictions_6))

      # evaluate the confusion_matrix
      cf_matrix =confusion_matrix(y_test, predictions)
      tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()

      #*******************************************confusion matrix Plot

      confusion_matrix_plot(y_test,predictions_6,model_name_6,contamination,percentage_values)

      #************************************************


      file_name = '/content/'+ f'{model_name_6}' + '.pkl'

      import joblib
      with open(file_name, 'wb') as f:
        joblib.dump(neigh, f)

      # import pickle
      # with open(file_name, 'wb') as f:
      #   pickle.dump(clf, f)

      df_all.loc[numb]= [f"{model_name_6}",tn, fp, fn, tp, accuracy_6, precision_6,recall_6,
                         f1_score_6,"{:.3%}".format(auc_6),"{:.2f}".format(average_precision_6),"{:.2f}".format(train_time_knn_sklearn)]

      numb = len(df_all)+1
    
    else:
      pass


    #***********************************************************************XGB
    if 'XGB' in list_of_models:
      from xgboost import XGBClassifier
      import time

      print('*****************************************************************XGB_xgboost')

      if number_of_unique != None:

        df_2 = df.copy()
        #df_2_exist = True

        #remove columns with constant numbers or those columns with unique numbers of < number_of_unique
        cols = df_2.columns
        for i in range(len(cols)):
          if cols[i] != name_target:
            m = df_2[cols[i]].value_counts()
            m = np.array(m)
            if len(m) < number_of_unique:
              print(f'len cols {i}:',len(m), 'droped')
              #print('drope')
              column_name = cols[i]
              df_2=df_2.drop(columns= column_name)

        X_2, y_2= df_2.loc[:, df_2.columns!= name_target], df_2[name_target]
        X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size=0.3, random_state=120,stratify=y_2)



        model_name_7 = 'XGB'
        model = XGBClassifier(learning_rate = 0.05, n_estimators=300, max_depth=5)
        start = time.time()
        model.fit(X_train_2, y_train_2)

        # make predictions for test set
        y_test_pred = model.predict(X_test_2)
        stop = time.time()
        train_time_ugr16 = stop - start
        print(f"Training time: {stop - start}s")

      else:

        model_name_7 = 'XGB'
        model = XGBClassifier(learning_rate = 0.05, n_estimators=300, max_depth=5)
        start = time.time()
        model.fit(X_train, y_train)

        # make predictions for test set
        y_test_pred = model.predict(X_test)

        stop = time.time()
        train_time_ugr16 = stop - start
        print(f"Training time: {stop - start}s")

      #****************************************
      predictions = [round(value) for value in y_test_pred]
      accuracy = accuracy_score(y_test, predictions)
      #print("Accuracy: %.2f%%" % (accuracy * 100.0))
      accuracy_7 = accuracy * 100.0

      for i in range(0,len(predictions)):
        if predictions[i] > 0.5:
          predictions[i]=1
        else:
          predictions[i]=0

      predictions_7 =predictions

      #AUC score
      auc_7 = metrics.roc_auc_score(y_test, predictions_7)
      # "{:.3%}".format(auc_7)

      # AP score
      average_precision_7 = metrics.average_precision_score(y_test, predictions_7)

      # calculate prediction,recall, f1-score
      from sklearn.metrics import f1_score,recall_score,precision_score
      precision = precision_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      recall = recall_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score = f1_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score_7 = np.mean(f1_score)
      precision_7 = np.mean(precision)
      recall_7 = np.mean(recall)

      # evaluate the classification_report
      print(classification_report(y_test, predictions_7))

      # evaluate the confusion_matrix
      cf_matrix =confusion_matrix(y_test, predictions_7)
      tn, fp, fn, tp = confusion_matrix(y_test, predictions_7).ravel()

      #*******************************************confusion matrix Plot

      confusion_matrix_plot(y_test,predictions_7,model_name_7,contamination,percentage_values)

      #************************************************

      file_name = '/content/'+ f'{model_name_7}' + '.pkl'
      import joblib
      with open(file_name, 'wb') as f:
        joblib.dump(model, f)

      # import pickle
      # with open(file_name, 'wb') as f:
      #   pickle.dump(clf, f)

      df_all.loc[numb]= [f"{model_name_7}",tn, fp, fn, tp, accuracy_7, precision_7,recall_7,
                         f1_score_7,"{:.3%}".format(auc_7),"{:.2f}".format(average_precision_7),"{:.2f}".format(train_time_ugr16)]

      numb = len(df_all)+1

    else:
      pass


#***************************************************************SOM

    if 'SOM' in list_of_models:

      print('*****************************************************************SOM_sklearn_som')
      
      #from somlearn import SOM
      import time
      from sklearn_som.som import SOM

      num_row , num_colmn = X.shape
      model_name_8 = 'SOM_sklearn_som'
      X = np.array(X)


      # train som detector
      #som = SOM(n_columns=num_colmn, n_rows=num_row, random_state=1)
      som = SOM(m=2, n=1, dim=num_colmn)
      start = time.time()
      #y_test_pred_8 = som.fit_predict(X)
      som.fit(X)

      stop = time.time()
      train_time_som = stop - start
      print(f"Training time: {stop - start}s")

      # make predictions for test set
      y_pred_8 = som.predict(X)      

      #*****************************************************
      predictions = [round(value) for value in y_pred_8]
      accuracy = accuracy_score(y, predictions)
      #print("Accuracy: %.2f%%" % (accuracy * 100.0))
      accuracy_8 = accuracy * 100.0

      for i in range(0,len(predictions)):
        if predictions[i] > 0.5:
          predictions[i]=1
        else:
          predictions[i]=0

      predictions_8 = predictions

      #AUC score
      auc_8 = metrics.roc_auc_score(y, predictions_8)
      # "{:.3%}".format(auc_6)

      # AP score
      average_precision_8 = metrics.average_precision_score(y, predictions_8)

      # calculate prediction,recall, f1-score
      from sklearn.metrics import f1_score,recall_score,precision_score
      precision = precision_score(y, predictions, average='weighted', labels=np.unique(predictions))
      recall = recall_score(y, predictions, average='weighted', labels=np.unique(predictions))
      f1_score = f1_score(y, predictions, average='weighted', labels=np.unique(predictions))
      f1_score_8 = np.mean(f1_score)
      precision_8 = np.mean(precision)
      recall_8 = np.mean(recall)

      # evaluate the classification_report
      print(classification_report(y, predictions_8))

      # evaluate the confusion_matrix
      cf_matrix =confusion_matrix(y, predictions)
      tn, fp, fn, tp = confusion_matrix(y, predictions).ravel()

      #*******************************************confusion matrix Plot

      confusion_matrix_plot(y,predictions_8,model_name_8,contamination,percentage_values)

      #************************************************


      file_name = '/content/'+ f'{model_name_8}' + '.pkl'

      import joblib
      with open(file_name, 'wb') as f:
        joblib.dump(som, f)

      # import pickle
      # with open(file_name, 'wb') as f:
      #   pickle.dump(clf, f)

      df_all.loc[numb]= [f"{model_name_8}",tn, fp, fn, tp, accuracy_8, precision_8,recall_8,
                         f1_score_8,"{:.3%}".format(auc_8),"{:.2f}".format(average_precision_8),"{:.2f}".format(train_time_som)]

      numb = len(df_all)+1
    
    else:
      pass  


    #**********************************************************************LOF
    if 'LOF_sklearn' in list_of_models:

      print('*****************************************************************LOF from sklearn lib')
      
      from sklearn.neighbors import LocalOutlierFactor
      import time

      model_name_9 = 'LOF_sklearn'
      # train knn detector
      neigh = LocalOutlierFactor(n_neighbors=3,novelty=True, contamination=contamination)
      start = time.time()
      neigh.fit(X_train)

      # get the prediction on the test data
      y_test_pred_9 = -neigh.predict(X_test)

      stop = time.time()
      train_time_LOF_sklearn = stop - start
      print(f"Training time: {stop - start}s")

      #*****************************************************
      predictions = [round(value) for value in y_test_pred_9]
      accuracy = accuracy_score(y_test, predictions)
      #print("Accuracy: %.2f%%" % (accuracy * 100.0))
      accuracy_9 = accuracy * 100.0

      for i in range(0,len(predictions)):
        if predictions[i] > 0.5:
          predictions[i]=1
        else:
          predictions[i]=0

      predictions_9 = predictions

      #AUC score
      auc_9 = metrics.roc_auc_score(y_test, predictions_9)
      # "{:.3%}".format(auc_6)

      # AP score
      average_precision_9 = metrics.average_precision_score(y_test, predictions_9)

      # calculate prediction,recall, f1-score
      from sklearn.metrics import f1_score,recall_score,precision_score
      precision = precision_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      recall = recall_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score = f1_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score_9 = np.mean(f1_score)
      precision_9 = np.mean(precision)
      recall_9 = np.mean(recall)

      # evaluate the classification_report
      print(classification_report(y_test, predictions_9))

      # evaluate the confusion_matrix
      cf_matrix =confusion_matrix(y_test, predictions)
      tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()

      #*******************************************confusion matrix Plot

      confusion_matrix_plot(y_test,predictions_9,model_name_9,contamination,percentage_values)

      #************************************************


      file_name = '/content/'+ f'{model_name_9}' + '.pkl'

      import joblib
      with open(file_name, 'wb') as f:
        joblib.dump(neigh, f)

      # import pickle
      # with open(file_name, 'wb') as f:
      #   pickle.dump(clf, f)

      df_all.loc[numb]= [f"{model_name_9}",tn, fp, fn, tp, accuracy_9, precision_9,recall_9,
                         f1_score_9,"{:.3%}".format(auc_9),"{:.2f}".format(average_precision_9),"{:.2f}".format(train_time_LOF_sklearn)]

      numb = len(df_all)+1
    
    else:
      pass


    #*********************************************************************OneClassSVM

    if 'OneClassSVM' in list_of_models:

      print('*****************************************************************OneClassSVM')
      
      from sklearn.svm import OneClassSVM
      import time

      model_name_10 = 'OneClassSVM'
      # train knn detector
      clf = OneClassSVM(gamma='auto')
      start = time.time()
      clf.fit(X_train)

      # get the prediction on the test data
      y_test_pred_10 = clf.predict(X_test)
      y_test_scores_oneclasssvm = clf.decision_function(X_test)  # outlier scores

      stop = time.time()
      train_time_oneclasssvm = stop - start
      print(f"Training time: {stop - start}s")

      #*****************************************************
      predictions = [round(value) for value in y_test_pred_10]
      accuracy = accuracy_score(y_test, predictions)
      #print("Accuracy: %.2f%%" % (accuracy * 100.0))
      accuracy_10 = accuracy * 100.0

      for i in range(0,len(predictions)):
        if predictions[i] > 0.5:
          predictions[i]=1
        else:
          predictions[i]=0

      predictions_10 = predictions

      #AUC score
      auc_10 = metrics.roc_auc_score(y_test, predictions_10)
      # "{:.3%}".format(auc_6)

      # AP score
      average_precision_10 = metrics.average_precision_score(y_test, predictions_10)

      # calculate prediction,recall, f1-score
      from sklearn.metrics import f1_score,recall_score,precision_score
      precision = precision_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      recall = recall_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score = f1_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score_10 = np.mean(f1_score)
      precision_10 = np.mean(precision)
      recall_10 = np.mean(recall)

      # evaluate the classification_report
      print(classification_report(y_test, predictions_10))

      # evaluate the confusion_matrix
      cf_matrix =confusion_matrix(y_test, predictions)
      tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()

      #*******************************************confusion matrix Plot

      confusion_matrix_plot(y_test,predictions_10,model_name_10,contamination,percentage_values)

      #************************************************


      file_name = '/content/'+ f'{model_name_10}' + '.pkl'

      import joblib
      with open(file_name, 'wb') as f:
        joblib.dump(clf, f)


      df_all.loc[numb]= [f"{model_name_10}",tn, fp, fn, tp, accuracy_10, precision_10,recall_10,
                         f1_score_10,"{:.3%}".format(auc_10),"{:.2f}".format(average_precision_10),"{:.2f}".format(train_time_oneclasssvm)]

      numb = len(df_all)+1
    
    else:
      pass


#********************************************************************OCSVM_pyod

    if 'OCSVM_pyod' in list_of_models:

      print('*****************************************************************OCSVM_pyod')
      
      #from sklearn.svm import OneClassSVM
      from pyod.models.ocsvm import OCSVM
      import time

      model_name_11 = 'OCSVM_pyod'
      # train knn detector
      clf = OCSVM(gamma='auto')
      start = time.time()
      clf.fit(X_train)

      # get the prediction on the test data
      y_test_pred_11 = clf.predict(X_test)
      y_test_scores_OCSVM_pyod = clf.decision_function(X_test)  # outlier scores

      stop = time.time()
      train_time_OCSVM_pyod = stop - start
      print(f"Training time: {stop - start}s")

      #*****************************************************
      predictions = [round(value) for value in y_test_pred_11]
      accuracy = accuracy_score(y_test, predictions)
      #print("Accuracy: %.2f%%" % (accuracy * 100.0))
      accuracy_11 = accuracy * 100.0

      for i in range(0,len(predictions)):
        if predictions[i] > 0.5:
          predictions[i]=1
        else:
          predictions[i]=0

      predictions_11 = predictions

      #AUC score
      auc_11 = metrics.roc_auc_score(y_test, predictions_11)
      # "{:.3%}".format(auc_6)

      # AP score
      average_precision_11 = metrics.average_precision_score(y_test, predictions_11)

      # calculate prediction,recall, f1-score
      from sklearn.metrics import f1_score,recall_score,precision_score
      precision = precision_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      recall = recall_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score = f1_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score_11 = np.mean(f1_score)
      precision_11 = np.mean(precision)
      recall_11 = np.mean(recall)

      # evaluate the classification_report
      print(classification_report(y_test, predictions_11))

      # evaluate the confusion_matrix
      cf_matrix =confusion_matrix(y_test, predictions)
      tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()

      #*******************************************confusion matrix Plot

      confusion_matrix_plot(y_test,predictions_11,model_name_11,contamination,percentage_values)

      #************************************************


      file_name = '/content/'+ f'{model_name_11}' + '.pkl'

      import joblib
      with open(file_name, 'wb') as f:
        joblib.dump(clf, f)


      df_all.loc[numb]= [f"{model_name_11}",tn, fp, fn, tp, accuracy_11, precision_11,recall_11,
                         f1_score_11,"{:.3%}".format(auc_11),"{:.2f}".format(average_precision_11),"{:.2f}".format(train_time_OCSVM_pyod)]

      numb = len(df_all)+1
    
    else:
      pass




#**********************************************************************SGDOneClassSVM


    if 'SGDOneClassSVM' in list_of_models:

      print('*****************************************************************SGDOneClassSVM')
      
      from sklearn.linear_model import SGDOneClassSVM
      import time

      model_name_12 = 'SGDOneClassSVM'
      # train knn detector
      clf = SGDOneClassSVM()
      start = time.time()
      clf.fit(X_train)

      # get the prediction on the test data
      y_test_pred_12 = clf.predict(X_test)
      y_test_scores_SGDOneClassSVM = clf.decision_function(X_test)  # outlier scores

      stop = time.time()
      train_time_SGDOneClassSVM = stop - start
      print(f"Training time: {stop - start}s")

      #*****************************************************
      predictions = [round(value) for value in y_test_pred_12]
      accuracy = accuracy_score(y_test, predictions)
      #print("Accuracy: %.2f%%" % (accuracy * 100.0))
      accuracy_12 = accuracy * 100.0

      for i in range(0,len(predictions)):
        if predictions[i] > 0.5:
          predictions[i]=1
        else:
          predictions[i]=0

      predictions_12 = predictions

      #AUC score
      auc_12 = metrics.roc_auc_score(y_test, predictions_12)
      # "{:.3%}".format(auc_6)

      # AP score
      average_precision_12 = metrics.average_precision_score(y_test, predictions_12)

      # calculate prediction,recall, f1-score
      from sklearn.metrics import f1_score,recall_score,precision_score
      precision = precision_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      recall = recall_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score = f1_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score_12 = np.mean(f1_score)
      precision_12 = np.mean(precision)
      recall_12 = np.mean(recall)

      # evaluate the classification_report
      print(classification_report(y_test, predictions_12))

      # evaluate the confusion_matrix
      cf_matrix =confusion_matrix(y_test, predictions)
      tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()

      #*******************************************confusion matrix Plot

      confusion_matrix_plot(y_test,predictions_12,model_name_12,contamination,percentage_values)

      #************************************************


      file_name = '/content/'+ f'{model_name_12}' + '.pkl'

      import joblib
      with open(file_name, 'wb') as f:
        joblib.dump(clf, f)


      df_all.loc[numb]= [f"{model_name_12}",tn, fp, fn, tp, accuracy_12, precision_12,recall_12,
                         f1_score_12,"{:.3%}".format(auc_12),"{:.2f}".format(average_precision_12),"{:.2f}".format(train_time_SGDOneClassSVM)]

      numb = len(df_all)+1
    
    else:
      pass





#***************************************************************************mcd

    if 'MCD_pyod' in list_of_models:

      print('*****************************************************************MCD_pyod')
      
      from pyod.models.mcd import MCD
      import time

      model_name_13 = 'MCD_pyod'
      # train knn detector

      num_row , num_column = X_train.shape
      
      sup_frac = float((num_row+num_column+1) /2)

      clf = MCD(contamination= contamination,support_fraction=sup_frac)
      start = time.time()
      clf.fit(X_train)

      # get the prediction on the test data
      y_test_pred_13 = clf.predict(X_test)
      y_test_scores_mcd = clf.decision_function(X_test)  # outlier scores

      stop = time.time()
      train_time_mcd = stop - start
      print(f"Training time: {stop - start}s")

      #*****************************************************
      predictions = [round(value) for value in y_test_pred_11]
      accuracy = accuracy_score(y_test, predictions)
      #print("Accuracy: %.2f%%" % (accuracy * 100.0))
      accuracy_13 = accuracy * 100.0

      for i in range(0,len(predictions)):
        if predictions[i] > 0.5:
          predictions[i]=1
        else:
          predictions[i]=0

      predictions_13 = predictions

      #AUC score
      auc_13 = metrics.roc_auc_score(y_test, predictions_13)


      # AP score
      average_precision_13 = metrics.average_precision_score(y_test, predictions_13)

      # calculate prediction,recall, f1-score
      from sklearn.metrics import f1_score,recall_score,precision_score
      precision = precision_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      recall = recall_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score = f1_score(y_test, predictions, average='weighted', labels=np.unique(predictions))
      f1_score_13 = np.mean(f1_score)
      precision_13 = np.mean(precision)
      recall_13 = np.mean(recall)

      # evaluate the classification_report
      print(classification_report(y_test, predictions_13))

      # evaluate the confusion_matrix
      cf_matrix =confusion_matrix(y_test, predictions)
      tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()

      #*******************************************confusion matrix Plot

      confusion_matrix_plot(y_test,predictions_13,model_name_13,contamination,percentage_values)

      #************************************************


      file_name = '/content/'+ f'{model_name_13}' + '.pkl'

      import joblib
      with open(file_name, 'wb') as f:
        joblib.dump(clf, f)


      df_all.loc[numb]= [f"{model_name_13}",tn, fp, fn, tp, accuracy_13, precision_13,recall_13,
                         f1_score_13,"{:.3%}".format(auc_13),"{:.2f}".format(average_precision_13),"{:.2f}".format(train_time_mcd)]

      numb = len(df_all)+1
    
    else:
      pass











    #*************************************Top N data plot

    plot_top_n(df,n,name_target,contamination,number_of_unique,list_of_models)

    #*************************************ROC PLOT

    plt.figure(0).clf()

    if 'HBOS_pyod' in list_of_models:
      fpr, tpr, thresh = metrics.roc_curve(y_test, predictions_1)
      auc = metrics.roc_auc_score(y_test, predictions_1)
      plt.plot(fpr,tpr,marker='.',label="HBOS_pyod, auc="+str("{:.3%}".format(auc)))

    if 'KNN_pyod' in list_of_models:
      fpr, tpr, thresh = metrics.roc_curve(y_test, predictions_2)
      auc = metrics.roc_auc_score(y_test, predictions_2)
      plt.plot(fpr,tpr,marker='.',label="KNN_pyod, auc="+str("{:.3%}".format(auc)))

    if 'IForest_pyod' in list_of_models:
      fpr, tpr, thresh = metrics.roc_curve(y_test, predictions_3)
      auc = metrics.roc_auc_score(y_test, predictions_3)
      plt.plot(fpr,tpr,marker='.',label="IForest_pyod, auc="+str("{:.3%}".format(auc)))

    if 'LOF_pyod' in list_of_models:
      fpr, tpr, thresh = metrics.roc_curve(y_test, predictions_4)
      auc = metrics.roc_auc_score(y_test, predictions_4)
      plt.plot(fpr,tpr,marker='.',label="LOF_pyod, auc="+str("{:.3%}".format(auc)))
      
    if 'CBLOF_pyod' in list_of_models:
      fpr, tpr, thresh = metrics.roc_curve(y_test, predictions_4cblof)
      auc = metrics.roc_auc_score(y_test, predictions_4cblof)
      plt.plot(fpr,tpr,marker='.',label="CBLOF_pyod, auc="+str("{:.3%}".format(auc)))

    if 'XBOS' in list_of_models: 
      if number_of_unique != None:
        fpr, tpr, thresh = metrics.roc_curve(y_test_2, predictions_5)
        auc = metrics.roc_auc_score(y_test_2, predictions_5)
        plt.plot(fpr,tpr,marker='.',label="XBOS, auc="+str("{:.3%}".format(auc)))

      else:
        fpr, tpr, thresh = metrics.roc_curve(y_test, predictions_5)
        auc = metrics.roc_auc_score(y_test, predictions_5)
        plt.plot(fpr,tpr,marker='.',label="XBOS, auc="+str("{:.3%}".format(auc)))

    if 'HBOS' in list_of_models: 
      if number_of_unique != None:
        fpr, tpr, thresh = metrics.roc_curve(y_test_2, predictions_5h)
        auc = metrics.roc_auc_score(y_test_2, predictions_5h)
        plt.plot(fpr,tpr,marker='.',label="HBOS, auc="+str("{:.3%}".format(auc)))

      else:
        fpr, tpr, thresh = metrics.roc_curve(y_test, predictions_5h)
        auc = metrics.roc_auc_score(y_test, predictions_5h)
        plt.plot(fpr,tpr,marker='.',label="HBOS, auc="+str("{:.3%}".format(auc)))

    if 'KNN_sklearn' in list_of_models:
      fpr, tpr, thresh = metrics.roc_curve(y_test, predictions_6)
      auc = metrics.roc_auc_score(y_test, predictions_6)
      plt.plot(fpr,tpr,marker='.',label="KNN_sklearn, auc="+str("{:.3%}".format(auc)))

    if 'XGB' in list_of_models:
      if number_of_unique != None:
        fpr, tpr, thresh = metrics.roc_curve(y_test_2, predictions_7)
        auc = metrics.roc_auc_score(y_test_2, predictions_7)
        plt.plot(fpr,tpr,marker='.',label="XGB, auc="+str("{:.3%}".format(auc)))

      else:
        fpr, tpr, thresh = metrics.roc_curve(y_test, predictions_7)
        auc = metrics.roc_auc_score(y_test, predictions_7)
        plt.plot(fpr,tpr,marker='.',label="XGB, auc="+str("{:.3%}".format(auc)))

    if 'SOM' in list_of_models:
      fpr, tpr, thresh = metrics.roc_curve(y, predictions_8)
      auc = metrics.roc_auc_score(y, predictions_8)
      plt.plot(fpr,tpr,marker='.',label="SOM, auc="+str("{:.3%}".format(auc)))


    if 'LOF_sklearn' in list_of_models:
      fpr, tpr, thresh = metrics.roc_curve(y_test, predictions_9)
      auc = metrics.roc_auc_score(y_test, predictions_9)
      plt.plot(fpr,tpr,marker='.',label="LOF_sklearn, auc="+str("{:.3%}".format(auc)))


    if 'OneClassSVM' in list_of_models:
      fpr, tpr, thresh = metrics.roc_curve(y_test, predictions_10)
      #print('thresh',thresh)
      auc = metrics.roc_auc_score(y_test, predictions_10)
      plt.plot(fpr,tpr,marker='.',label="OneClassSVM, auc="+str("{:.3%}".format(auc)))


    if 'OCSVM_pyod' in list_of_models:
      fpr, tpr, thresh = metrics.roc_curve(y_test, predictions_11)
      auc = metrics.roc_auc_score(y_test, predictions_11)
      plt.plot(fpr,tpr,marker='.',label="OCSVM_pyod, auc="+str("{:.3%}".format(auc)))

    if 'SGDOneClassSVM' in list_of_models:
      fpr, tpr, thresh = metrics.roc_curve(y_test, predictions_12)
      auc = metrics.roc_auc_score(y_test, predictions_12)
      plt.plot(fpr,tpr,marker='.',label="SGDOneClassSVM, auc="+str("{:.3%}".format(auc)))


    if 'MCD_pyod' in list_of_models:
      fpr, tpr, thresh = metrics.roc_curve(y_test, predictions_13)
      auc = metrics.roc_auc_score(y_test, predictions_13)
      plt.plot(fpr,tpr,marker='.',label="MCD_pyod, auc="+str("{:.3%}".format(auc)))


    plt.title('ROC-Curve')
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')
    #plt.legend(loc=0)
    plt.legend(bbox_to_anchor=(1.04,1), loc="upper left")
    plt.show()

    #************************************************ Plot_precision_recall_curve

    from sklearn.metrics import precision_recall_curve
    from sklearn.metrics import plot_precision_recall_curve

    if 'HBOS_pyod' in list_of_models:
      disp_1 = my_plot_precision_recall_curve(y_test, predictions_1,"HBOS_pyod")
      average_precision_1 = metrics.average_precision_score(y_test, y_test_scores_hbos)
      disp_1.ax_.set_title('2-class Precision-Recall curve')


    if 'KNN_pyod' in list_of_models:
      disp_2 = my_plot_precision_recall_curve(y_test, predictions_2,"KNN_pyod")
      average_precision_2 = metrics.average_precision_score(y_test, y_test_scores_knn)
      disp_2.ax_.set_title('2-class Precision-Recall curve')


    if 'IForest_pyod' in list_of_models:
      disp_3 = my_plot_precision_recall_curve(y_test, predictions_3,"IForest_pyod")
      average_precision_3 = metrics.average_precision_score(y_test, y_test_scores_iforest)
      disp_3.ax_.set_title('2-class Precision-Recall curve')


    if 'LOF_pyod' in list_of_models:
      disp_4 = my_plot_precision_recall_curve(y_test, predictions_4,"LOF_pyod")
      average_precision_4 = metrics.average_precision_score(y_test, y_test_scores_lof)
      disp_4.ax_.set_title('2-class Precision-Recall curve')

    if 'LOF_pyod' in list_of_models:
      disp_4 = my_plot_precision_recall_curve(y_test, predictions_4cblof,"CBLOF_pyod")
      average_precision_4 = metrics.average_precision_score(y_test, y_test_scores_cblof)
      disp_4.ax_.set_title('2-class Precision-Recall curve')
      
    if 'XBOS' in list_of_models:
      if number_of_unique != None:
        disp_5 = my_plot_precision_recall_curve(y_test_2, predictions_5,"XBOS")
        average_precision_5 = metrics.average_precision_score(y_test_2, y_test_scores_xbos)
        disp_5.ax_.set_title('2-class Precision-Recall curve')


      else:
        disp_5 = my_plot_precision_recall_curve(y_test, predictions_5,"XBOS")
        average_precision_5 = metrics.average_precision_score(y_test, y_test_scores_xbos)
        disp_5.ax_.set_title('2-class Precision-Recall curve')

    if 'HBOS' in list_of_models:
      if number_of_unique != None:
        disp_5 = my_plot_precision_recall_curve(y_test_2, predictions_5h,"HBOS")
        average_precision_5 = metrics.average_precision_score(y_test_2, y_test_scores_hbos)
        disp_5.ax_.set_title('2-class Precision-Recall curve')


      else:
        disp_5 = my_plot_precision_recall_curve(y_test, predictions_5h,"HBOS")
        average_precision_5 = metrics.average_precision_score(y_test, y_test_scores_hbos)
        disp_5.ax_.set_title('2-class Precision-Recall curve')
        
    if 'KNN_sklearn' in list_of_models:
      disp_6 = my_plot_precision_recall_curve(y_test, predictions_6,"KNN_sklearn")
      average_precision_6 = metrics.average_precision_score(y_test, y_test_pred_6)
      disp_6.ax_.set_title('2-class Precision-Recall curve')

    if 'XGB' in list_of_models:
      if number_of_unique != None:
        disp_7 = my_plot_precision_recall_curve(y_test_2, predictions_7,"XGB")
        average_precision_7 = metrics.average_precision_score(y_test_2, predictions_7)
        disp_7.ax_.set_title('2-class Precision-Recall curve')

      else:
        disp_7 = my_plot_precision_recall_curve(y_test, predictions_7,"XGB")
        average_precision_7 = metrics.average_precision_score(y_test, predictions_7)
        disp_7.ax_.set_title('2-class Precision-Recall curve')


    if 'SOM' in list_of_models:
      disp_8 = my_plot_precision_recall_curve(y, predictions_8,"SOM")
      average_precision_8 = metrics.average_precision_score(y, predictions_8)
      disp_8.ax_.set_title('2-class Precision-Recall curve')

    if 'LOF_sklearn' in list_of_models:
      disp_9 = my_plot_precision_recall_curve(y_test, predictions_9,"LOF_sklearn")
      average_precision_9 = metrics.average_precision_score(y_test, predictions_9)
      disp_9.ax_.set_title('2-class Precision-Recall curve')


    if 'OneClassSVM' in list_of_models:
      disp_10 = my_plot_precision_recall_curve(y_test, predictions_10,"OneClassSVM")
      average_precision_10 = metrics.average_precision_score(y_test, predictions_10)
      disp_10.ax_.set_title('2-class Precision-Recall curve')

    if 'OCSVM_pyod' in list_of_models:
      disp_11 = my_plot_precision_recall_curve(y_test, predictions_11,"OCSVM_pyod")
      average_precision_11 = metrics.average_precision_score(y_test, predictions_11)
      disp_11.ax_.set_title('2-class Precision-Recall curve')

    if 'SGDOneClassSVM' in list_of_models:
      disp_12 = my_plot_precision_recall_curve(y_test, predictions_12,"SGDOneClassSVM")
      average_precision_12 = metrics.average_precision_score(y_test, predictions_12)
      disp_12.ax_.set_title('2-class Precision-Recall curve')


    if 'MCD_pyod' in list_of_models:
      disp_13 = my_plot_precision_recall_curve(y_test, predictions_13,"MCD_pyod")
      average_precision_13 = metrics.average_precision_score(y_test, predictions_13)
      disp_13.ax_.set_title('2-class Precision-Recall curve')


    plt.legend(bbox_to_anchor=(1.04,1), loc="upper left")
    plt.show()


    #*********************************#data frame table
    fig = plt.figure(figsize = (20, 9))
    ax = fig.add_subplot(111)

    t = ax.table(cellText = df_all.values,
              rowLabels = df_all.index,
              colLabels = df_all.columns,
              loc = "center",
              #colWidths = [0.9]*len(df_all.columns),
            )
    t.auto_set_font_size(False) 
    t.set_fontsize(9)
    ax.set_title(dataset_name)
    ax.axis("off")
    fig.savefig("/content/test.png")


    return df_all




