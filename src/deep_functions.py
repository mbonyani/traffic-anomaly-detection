# -*- coding: utf-8 -*-
"""deep_functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rd_Sk51XNepHgamU37Im8SyzGSxVW_pI
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split 
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay

import matplotlib.pyplot as plt
from sklearn import metrics
import os
import seaborn as sns
import time


from numpy import vstack
from numpy import sqrt
from pandas import read_csv
from sklearn.metrics import mean_squared_error
from torch.utils.data import random_split

import torch
import deep_classes




def compute_train(X_train,y_train,shape,EPOCH,MODEL_SELECTED,PATH,DATASETNAME):
    """
        Computation : Find Anomaly using model based computation 
    """
    if str(MODEL_SELECTED) == "LSTM-AE":
        model = deep_classes.LSTMAE(shape,shape) #df.shape[1]
        criterion = torch.nn.MSELoss(reduction='mean')
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
        train_data = torch.utils.data.TensorDataset(torch.tensor(X_train.astype(np.float32)), torch.tensor(X_train.astype(np.float32)))
        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=32, shuffle=False)
        train_step = make_train_step(model, criterion, optimizer)
        
        start = time.time()
        loss_train_lstmae = []
        for epoch in range(EPOCH):
            loss_sum = 0.0
            ctr = 0
            for x_batch, y_batch in train_loader:
                loss_train = train_step(x_batch, y_batch)
                loss_sum += loss_train
                ctr += 1
            print("Training Loss: {0} - Epoch: {1}".format(float(loss_sum/ctr), epoch+1))
            loss_plot_epoch = float(loss_sum/ctr)
            loss_train_lstmae.append(loss_plot_epoch)

            # save model
            torch.save(model.state_dict(), PATH)
        
        stop = time.time()
        train_time_lstmae = stop - start
        print(f"Training time: {stop - start}s")

        hypothesis = model(torch.tensor(X_train.astype(np.float32))).detach().numpy()
        #print(hypothesis.shape)
        loss = np.linalg.norm(hypothesis - X_train, axis= (1,2))#(1,2)

        return loss.reshape(len(loss),1),train_time_lstmae,loss_train_lstmae



    elif str(MODEL_SELECTED) == "DeepAnT":#***********************
        model = deep_classes.DeepAnT(shape,shape,DATASETNAME)
        criterion = torch.nn.MSELoss(reduction='mean')
        optimizer = torch.optim.Adam(list(model.parameters()), lr=1e-5)
        train_data = torch.utils.data.TensorDataset(torch.tensor(X_train.astype(np.float32)), torch.tensor(y_train.astype(np.float32)))
        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=32, shuffle=False)
        train_step = make_train_step(model, criterion, optimizer)

        start = time.time()
        loss_train_deepant = []
        for epoch in range(EPOCH):
            loss_sum = 0.0
            ctr = 0
            for x_batch, y_batch in train_loader:
                loss_train = train_step(x_batch, y_batch)
                loss_sum += loss_train
                ctr += 1
            print("Training Loss: {0} - Epoch: {1}".format(float(loss_sum/ctr), epoch+1))
            loss_plot_epoch = float(loss_sum/ctr)
            loss_train_deepant.append(loss_plot_epoch)

        stop = time.time()
        train_time_deepant = stop - start
        print(f"Training time: {stop - start}s")

        # save model
        torch.save(model.state_dict(), PATH)

        hypothesis = model(torch.tensor(X_train.astype(np.float32))).detach().numpy()
        loss = np.linalg.norm(hypothesis - y_train, axis=1)

        
        return loss.reshape(len(loss),1),train_time_deepant,loss_train_deepant
    else:
        print("Selection of Model is not in the set")
        return None


def make_train_step(model, loss_fn, optimizer):
    """
        Computation : Function to make batch size data iterator
    """
    def train_step(x, y):
        model.train()
        yhat = model(x)
        loss = loss_fn(y, yhat)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        return loss.item()
    return train_step


#**********************************************************

def compute_test(X_test,y_test,model,EPOCH,MODEL_SELECTED):
    """
        Computation : Find Anomaly using model based computation 
    """
    if str(MODEL_SELECTED) == "LSTM-AE": #*****************
        #model = LSTMAE(df.shape[1],df.shape[1])
        model=model
        criterion = torch.nn.MSELoss(reduction='mean')
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
        test_data = torch.utils.data.TensorDataset(torch.tensor(X_test.astype(np.float32)), torch.tensor(X_test.astype(np.float32)))
        test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=32, shuffle=False)
        test_step = make_test_step(model, criterion, optimizer)
        #print(train_step)

        start = time.time()
        loss_test_lstmae = []

        for epoch in range(EPOCH):
            loss_sum = 0.0
            ctr = 0
            for x_batch, y_batch in test_loader:
                loss_test = test_step(x_batch, y_batch)                
                loss_sum += loss_test                
                ctr += 1
            print("Testing Loss: {0} - Epoch: {1}".format(float(loss_sum/ctr), epoch+1))
            loss_plot_epoch = float(loss_sum/ctr)
            loss_test_lstmae.append(loss_plot_epoch)

        stop = time.time()
        test_time_lastmae = stop - start
        print(f"Testing time: {stop - start}s")

        hypothesis = model(torch.tensor(X_test.astype(np.float32))).detach().numpy()
        #print(hypothesis.shape)
        loss = np.linalg.norm(hypothesis - X_test, axis=(1,2))

        return loss.reshape(len(loss),1),loss_test_lstmae
        # save model
        #torch.save(model.state_dict(), PATH)



    elif str(MODEL_SELECTED) == "DeepAnT":#***********************
        #model = DeepAnT(31,31)
        model= model
        criterion = torch.nn.MSELoss(reduction='mean')
        optimizer = torch.optim.Adam(list(model.parameters()), lr=1e-5)
        test_data = torch.utils.data.TensorDataset(torch.tensor(X_test.astype(np.float32)), torch.tensor(y_test.astype(np.float32)))
        test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=32, shuffle=False)
        test_step = make_test_step(model, criterion, optimizer)

        # test_data = torch.utils.data.TensorDataset(torch.tensor(X_test.astype(np.float32)), torch.tensor(y_test.astype(np.float32)))
        # test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=32, shuffle=False)
        #test_step = make_train_step(model, criterion, optimizer)
            
        start = time.time()
        loss_test_deepant = []

        for epoch in range(EPOCH):
            loss_sum = 0.0
            ctr = 0
            for x_batch, y_batch in test_loader:
                loss_test = test_step(x_batch, y_batch)
                loss_sum += loss_test
                ctr += 1
            print("Testing Loss: {0} - Epoch: {1}".format(float(loss_sum/ctr), epoch+1))
            loss_plot_epoch = float(loss_sum/ctr)
            loss_test_deepant.append(loss_plot_epoch)

        stop = time.time()
        test_time_deepant = stop - start
        print(f"Testing time: {stop - start}s")

        hypothesis = model(torch.tensor(X_test.astype(np.float32))).detach().numpy()
        loss = np.linalg.norm(hypothesis - y_test, axis=1)
        #print(loss.shape)
        # save model
        #torch.save(model.state_dict(), PATH)
        
        return loss.reshape(len(loss),1),loss_test_deepant
    else:
        print("Selection of Model is not in the set")
        return None


def make_test_step(model, loss_fn, optimizer):
    """
        Computation : Function to make batch size data iterator
    """
    def test_step(x, y):
        #model.train()
        yhat = model(x)
        loss = loss_fn(y, yhat)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        return loss.item()
    return test_step



def plot_top_n_deepmodels(df,n,name_target,loss_total_1,loss_total_2):


    from sklearn import metrics

    orig = df.copy()
    results = []

    #*********************************************

    results.append(loss_total_1)
    results.append(loss_total_2)

    #*****************************************

    deepant_result = results[0]
    deepant_orig = orig.copy()
    #hbos_orig[titles[i]] = hbos_result
    deepant_orig['DeepAnT'] = deepant_result
    deepant_topN_data = deepant_orig.sort_values(by=['DeepAnT'],ascending=False)[:n]

    lstmae_result = results[1]
    lstmae_orig = orig.copy()
    lstmae_orig['LSTM-AE'] = lstmae_result
    lstmae_topN_data = lstmae_orig.sort_values(by=['LSTM-AE'],ascending=False)[:n]
    #print(len(knn_topN_data[lambda x:x[name_target]==1]))


    from matplotlib import pyplot as plt
    plt.scatter(range(n),deepant_topN_data[name_target].cumsum(),marker='1',label='DeepAnT')
    plt.scatter(range(n),lstmae_topN_data[name_target].cumsum(),marker='1',label='LSTM-AE')         

    plt.xlabel('Top N data')
    plt.ylabel('Anomalies found')
    plt.title('How many anomalies can we find in the top N data ( N=1... 1000 )? \n AUC-ish score') 
    plt.legend(bbox_to_anchor=(1.04,1), loc="upper left")
    plt.show()